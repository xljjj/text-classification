{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ea167a-006c-4d17-a95e-98a7854fb8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow-gpu==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6575f84b-baaa-4784-9fd8-ac07f297813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import moxing as mox\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86247fd1-ad6e-4300-b4d8-6b8b50fcf0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf9bfa45-aebc-4edf-9ea8-1b7b55a2aa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "113172de-7501-4cc0-8570-a665fe076030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_DIR为训练集根目录，这里设置为桶的dataset目录\n",
    "BASE_DIR = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a727f49-fefe-4dc8-ad5c-1fcd756fe233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "./data/20_newsgroup\n"
     ]
    }
   ],
   "source": [
    "# 文本语料路径\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# 存储词向量到字典中\n",
    "print('Indexing word vectors.')\n",
    "print(TEXT_DATA_DIR)\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(BASE_DIR, 'glove.6B.100d.txt'), 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fad3572-bd10-4456-83aa-a08928533892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19997 texts.\n"
     ]
    }
   ],
   "source": [
    "#将每篇文章的文件名和标签进行存储\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    i = t.find('\\n\\n')  # skip header\n",
    "                    if 0 < i:\n",
    "                        t = t[i:]\n",
    "                    texts.append(t)\n",
    "                labels.append(label_id)\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f87d76e4-195a-4f7d-a8eb-8de4227c82ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['token_result.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#分词\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "joblib.dump(tokenizer, 'token_result.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bdd7c15-88aa-4d5e-916b-746455502c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  58  576    3 ...    4  930 2050]\n",
      " [ 221   31  972 ... 2932  552  324]\n",
      " [   0    0    0 ...    3  316 5816]\n",
      " ...\n",
      " [   0    0    0 ...   71  197  514]\n",
      " [   0    0    0 ... 2113 1618 9557]\n",
      " [   0    0    0 ...    3    1 2703]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "Shape of data tensor: (19997, 1000)\n",
      "Shape of label tensor: (19997, 20)\n",
      "[[    0     0     0 ...     4  1636   453]\n",
      " [    0     0     0 ... 13710     6 14246]\n",
      " [    0     0     0 ...  3554   344  2182]\n",
      " ...\n",
      " [    0     0     0 ...  5734 11553    26]\n",
      " [    0     0     0 ...  2433  3662   813]\n",
      " [    0     0     0 ...   439  4032  9247]]\n",
      "[[0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "19997\n"
     ]
    }
   ],
   "source": [
    "#数据打乱和划分\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(data)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "print(data)\n",
    "labels = labels[indices]\n",
    "print(labels)\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "print(data.shape[0])\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "461ba761-9ac9-42ec-bead-02bb82070602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "#数据降维\n",
    "print('Preparing embedding matrix.')\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        # 从预训练模型的词向量到语料库的词向量映射\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "print('Training model.')\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "050cf085-4ec9-4f7f-afa3-a5a4381aeef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(embedded_sequences.shape)\n",
    "#LSTM-CNN模型\n",
    "#首先通过Embedding Layer将单词转化为词向量\n",
    "#再输入LSTM进行语义特征提取\n",
    "#下一步将LSTM的输出作为CNN的输入\n",
    "#进行进一步的特征提取\n",
    "#最后得到分类结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0b01761-9c31-4161-9253-f100cd03dfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 256)\n"
     ]
    }
   ],
   "source": [
    "#LSTM层\n",
    "lstm_layer=LSTM(units=256,batch_size=128)\n",
    "lstm_sequences=lstm_layer(embedded_sequences)\n",
    "print(lstm_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49c8bd1c-3036-4753-80b1-866ee0396967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "# 卷积要求输入为3维\n",
    "lstm_sequences=tf.reshape(lstm_sequences,shape=[-1,256,1])\n",
    "print(lstm_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e69fc0ec-344b-40ee-addf-72a7ff7512b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 128, 256)\n"
     ]
    }
   ],
   "source": [
    "#CNN层\n",
    "cnn_layer=Conv1D(filters=256,kernel_size=129,padding='valid', activation=tf.nn.relu)\n",
    "cnn_sequences=cnn_layer(lstm_sequences)\n",
    "print(cnn_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb4da118-4446-46af-a8cc-3aceb1cb90ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64, 256)\n"
     ]
    }
   ],
   "source": [
    "#MaxPooling层\n",
    "maxPooling_layer=MaxPooling1D()\n",
    "maxPooling_sequences=maxPooling_layer(cnn_sequences)\n",
    "print(maxPooling_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9015ae50-211e-43a9-811b-4e79d7e18b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 16384)\n"
     ]
    }
   ],
   "source": [
    "#展平为1维\n",
    "maxPooling_sequences=tf.reshape(maxPooling_sequences,shape=[-1,64*256])\n",
    "maxPooling_sequences=tf.nn.dropout(maxPooling_sequences,rate=0.11)\n",
    "print(maxPooling_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "378d5561-54eb-43ac-a869-65921e411c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 4096)\n"
     ]
    }
   ],
   "source": [
    "dense_layer1=Dense(units=4096,activation=tf.nn.relu)\n",
    "dense_sequences1=dense_layer1(maxPooling_sequences)\n",
    "print(dense_sequences1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2f7668f-4fe5-4ba9-b26d-9c484f3527e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1024)\n"
     ]
    }
   ],
   "source": [
    "dense_layer2=Dense(units=1024,activation=tf.nn.relu)\n",
    "dense_sequences2=dense_layer2(dense_sequences1)\n",
    "print(dense_sequences2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fb13c06-3872-48e5-a866-5b9132441ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 20)\n"
     ]
    }
   ],
   "source": [
    "output_layer=Dense(units=labels.shape[1],activation='softmax')\n",
    "outputs=output_layer(dense_sequences2)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "698eec42-cdc5-4642-8db1-db06fa6331e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1000, 100)    2000000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          365568      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 256, 1)]     0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 128, 256)     33280       tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 64, 256)      0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 16384)]      0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RandomUniform (Tens [(None, 16384)]      0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_GreaterEqual (Tenso [(None, 16384)]      0           tf_op_layer_RandomUniform[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul (TensorFlowOpLa [(None, 16384)]      0           tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Cast (TensorFlowOpL [(None, 16384)]      0           tf_op_layer_GreaterEqual[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_1 (TensorFlowOp [(None, 16384)]      0           tf_op_layer_Mul[0][0]            \n",
      "                                                                 tf_op_layer_Cast[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 4096)         67112960    tf_op_layer_Mul_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         4195328     dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 20)           20500       dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 73,727,636\n",
      "Trainable params: 71,727,636\n",
      "Non-trainable params: 2,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs = sequence_input, outputs = outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cabeebfb-0156-4276-9150-060201424e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.losses.categorical_crossentropy,optimizer=tf.optimizers.RMSprop(learning_rate=0.005),metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf482655-415a-45fe-af1c-e164af252474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 20s 158ms/step - loss: 0.5375 - acc: 0.8175 - val_loss: 1.4660 - val_acc: 0.6782\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 20s 158ms/step - loss: 0.4883 - acc: 0.8338 - val_loss: 1.4815 - val_acc: 0.7057\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 20s 158ms/step - loss: 0.4690 - acc: 0.8498 - val_loss: 1.6598 - val_acc: 0.6992\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 20s 158ms/step - loss: 0.4190 - acc: 0.8633 - val_loss: 1.4644 - val_acc: 0.6979\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 20s 158ms/step - loss: 0.3975 - acc: 0.8707 - val_loss: 1.5808 - val_acc: 0.7082\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 20s 157ms/step - loss: 0.3697 - acc: 0.8773 - val_loss: 1.6560 - val_acc: 0.7214\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 20s 158ms/step - loss: 0.3665 - acc: 0.8833 - val_loss: 2.0890 - val_acc: 0.7107\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 20s 158ms/step - loss: 0.3529 - acc: 0.8904 - val_loss: 2.0226 - val_acc: 0.7147\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 20s 158ms/step - loss: 0.3383 - acc: 0.8980 - val_loss: 1.8644 - val_acc: 0.7197\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 20s 158ms/step - loss: 0.3131 - acc: 0.8987 - val_loss: 2.4505 - val_acc: 0.7157\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc8a3586-d2df-48f7-9d7a-cfc09412cfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk/home/ma-user/work/mytextcnn_model.h5\n"
     ]
    }
   ],
   "source": [
    "# 先在虚拟机上保存模型，再将模型拷贝至桶的输出路径下。\n",
    "Model_DIR = os.path.join(os.getcwd(), 'mytextcnn_model.h5')\n",
    "model.save(Model_DIR)\n",
    "print('Saved model to disk'+Model_DIR)\n",
    "# 第二个参数需要根据实验者的桶路径修改\n",
    "mox.file.copy_parallel(Model_DIR,'obs://nlp-textclassifier/output/mytextcnn_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-2.1",
   "language": "python",
   "name": "tensorflow-2.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
